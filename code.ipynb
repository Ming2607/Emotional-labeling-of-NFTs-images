{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desktop\n",
    "    datafile\n",
    "        NFT_All.csv\n",
    "        NFT_emotion.csv\n",
    "        positive_or_negative.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.text import Text\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import seaborn as sns\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the file to be read\n",
    "df = pd.read_spss(r\"/Users/Desktop/datafile/NFT_All.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The frequency of each person's emotional choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(df.iloc[:,15:733:3].apply(lambda x:x.value_counts().to_dict(),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select(cell,emo):\n",
    "    if emo in cell.keys():\n",
    "        return cell[emo]\n",
    "    else:\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"if_disgust\"] = df1.applymap(lambda x:select(x,emo=\"Disgust\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"if_Amusement\"] = df1.applymap(lambda x:select(x,emo=\"Amusement\"))\n",
    "df[\"if_Awe\"] = df1.applymap(lambda x:select(x,emo=\"Awe\"))\n",
    "df[\"if_Contentment\"] = df1.applymap(lambda x:select(x,emo=\"Contentment\"))\n",
    "df[\"if_Excitement\"] = df1.applymap(lambda x:select(x,emo=\"Excitement\"))\n",
    "df[\"if_Anger\"] = df1.applymap(lambda x:select(x,emo=\"Anger\"))\n",
    "df[\"if_Fear\"] = df1.applymap(lambda x:select(x,emo=\"Fear\"))\n",
    "df[\"if_Sandness\"] = df1.applymap(lambda x:select(x,emo=\"Sadness\"))\n",
    "df[\"other\"] =df1.applymap(lambda x:select(x,emo=\"Something else（Please specify which emotion it is below）\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data\n",
    "df.to_csv(r\"/Users/Desktop/datafile/1_expert.csv\",encoding=\"gbk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Count the number of choices for each emotion by item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_spss(r\"/Users/Desktop/datafile/NFT_emotion.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp =temp.iloc[:,777:885]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "columns_name_list = [[\"amusement\",\"awe\",\"contenment\",\"excitement\",\"anger\",\"disgust\",\"fear\",\"sandnedd\",\"other\"]]\n",
    "for i in range(0,len(temp.columns),9):\n",
    "    df_temp = temp[ \n",
    "        temp.iloc[:,i:i+9].notna().any(axis=1)\n",
    "    ].iloc[:,i:i+9]\n",
    "    result.append(df_temp.sum().to_list())\n",
    "\n",
    "result_df  = pd.DataFrame(result)\n",
    "result_df.columns=[[\"amusement\",\"awe\",\"contenment\",\"excitement\",\"anger\",\"disgust\",\"fear\",\"sandnedd\",\"other\"]]\n",
    "result_df.index=[i for i in range(1,13)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(r\"/Users/Desktop/datafile/2_expert.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The text of emotion interpretation is divided into words and word frequency is extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cipin = df.iloc[:,17:737:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = list(df_cipin.to_numpy().flatten()) \n",
    "sentence_list = [x for x in sentence_list if len(x)!=0] \n",
    "sentence_str = \".\".join(sentence_list) \n",
    "pd.DataFrame(sentence_list).to_csv(r\"/Users/Desktop/datafile/3_expert.csv\",encoding=\"GBK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_str = \" \".join(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_punctuation(text):\n",
    "    punctuations = string.punctuation\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in punctuations]\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "\n",
    "token = filter_punctuation(sentence_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = [word for word  in token if word.lower() not in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# 1. Acquired part of speech\n",
    "def get_wordnet_pos(tag): \n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# 2. Part of speech merging\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    lemmatized_tokens = []\n",
    "    for token, tag in tagged_tokens:\n",
    "        pos = get_wordnet_pos(tag)\n",
    "        lemmatized_token = lemmatizer.lemmatize(token, pos=pos)\n",
    "        lemmatized_tokens.append(lemmatized_token)\n",
    "    return lemmatized_tokens\n",
    "\n",
    "\n",
    "\n",
    "lem_result = lemmatize_tokens(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/mingyang/Desktop/paperneed/paper 1 已润色/paper1_code.ipynb 单元格 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mingyang/Desktop/paperneed/paper%201%20%E5%B7%B2%E6%B6%A6%E8%89%B2/paper1_code.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m t \u001b[39m=\u001b[39m Text(lem_result)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mingyang/Desktop/paperneed/paper%201%20%E5%B7%B2%E6%B6%A6%E8%89%B2/paper1_code.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m t\u001b[39m.\u001b[39mplot(\u001b[39m30\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Text' is not defined"
     ]
    }
   ],
   "source": [
    "t = Text(lem_result)\n",
    "t.plot(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Output word frequency and part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = FreqDist(lem_result)\n",
    "pd.DataFrame(\n",
    "   {\"word\":freq.keys(),\n",
    "    \"freq\":freq.values(),\n",
    "    \"type\":[i[1] for i in  pos_tag(freq.keys())]\n",
    "    }\n",
    ").to_csv(r\"/Users/Desktop/datafile/4_expert.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Eight key words affective tendency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_image  = pd.read_spss(r\"/Users/Desktop/datafile/NFT_All.sav\").iloc[:,15:735]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in df_image.columns:\n",
    "    if \"Other\" in i:\n",
    "        df_image.drop(labels=i,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    x = x.replace(\"color\",\"colour\").replace(\"Colours\",\"colour\").replace(\"colourful\",\"colour\").replace(\"colouring\",\"colour\").replace(\"multi-coloured\",\"colour\").replace(\"Color\",\"colour\").replace(\"colorful\",\"colour\").replace(\"EXPRESSION\",\"expression\").replace(\"eyes\",\"eye\").replace(\"EYES\",\"eye\").replace(\"Color\",\"colour\").replace(\"colorful\",\"colour\") .replace(\"EXPRESSION\",\"expression\").replace(\"eyes\",\"eye\").replace(\"skin\",\"reskin\").replace(\"hats\",\"hat\").replace(\"expressioj\",\"expression\").replace(\"hairs\",\"hair\").replace(\"mouth/nose\",\"other\").replace(\"mouthpiece\",\"other\")\n",
    "\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image.iloc[:,1::2] = df_image.iloc[:,1::2].applymap(lambda x:func(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_list = []\n",
    "i=0\n",
    "while i<len(df_image.columns) :\n",
    "    if i%2==0:\n",
    "        temp = df_image.iloc[:,i:i+2]\n",
    "        image_list.append(temp)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"colour\",\"expression\",\"eye\",\"mouth\",\"hair\",\"hat\",\"skin\",\"background\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def keyword_func(cell,keyword):\n",
    "    return keyword in cell\n",
    "    \n",
    "\n",
    "def calculate(x):\n",
    "    temp_list = []\n",
    "    for df  in x:\n",
    "        res = df[df.iloc[:,1] == True].iloc[:,0].value_counts()\n",
    "        temp_list.append(res)\n",
    "    return temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y8/mzg2yr0d4kg8xhb8s78tgd8w0000gn/T/ipykernel_10190/4238437664.py:3: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_image.iloc[:,1::2] = df_image.iloc[:,1::2].applymap(lambda x: keyword_func(x,\"mouth\"))\n"
     ]
    }
   ],
   "source": [
    "# Exporting different keyword sentiment data needs to be adjusted in this line of code\n",
    "df_image.iloc[:,1::2] = df_image.iloc[:,1::2].applymap(lambda x: keyword_func(x,\"mouth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = 0\n",
    "data_list  = []\n",
    "while i< len(df_image.columns):\n",
    "    if i%2==0:\n",
    "       data_list.append(df_image.iloc[:,i:i+2])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.DataFrame(calculate(data_list)).fillna(value=0).to_csv(r\"/Users/Desktop/datafile/mouth.csv\",encoding=\"GBK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Judgment of positive/negative emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_spss(r\"/Users/Desktop/datafile/NFT_All.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.iloc[:,15:734:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.replace({\n",
    "    \"Amusement\":1,\n",
    "    \"Awe\":1,\n",
    "    \"Contentment\":1,\n",
    "    \"Excitement\":1,\n",
    "    \"Anger\":-1,\n",
    "    \"Disgust\":-1,\n",
    "    \"Fear\":-1,\n",
    "    \"Sadness\":-1,\n",
    "    \"Something else（Please specify which emotion it is below）\":0,\n",
    "    \"nan\":0\n",
    "\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_1</th>\n",
       "      <th>Image_2</th>\n",
       "      <th>Image_3</th>\n",
       "      <th>Image_4</th>\n",
       "      <th>Image_5</th>\n",
       "      <th>Image_6</th>\n",
       "      <th>Image_7</th>\n",
       "      <th>Image_8</th>\n",
       "      <th>Image_9</th>\n",
       "      <th>Image_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Image_231</th>\n",
       "      <th>Image_232</th>\n",
       "      <th>Image_233</th>\n",
       "      <th>Image_234</th>\n",
       "      <th>Image_235</th>\n",
       "      <th>Image_236</th>\n",
       "      <th>Image_237</th>\n",
       "      <th>Image_238</th>\n",
       "      <th>Image_239</th>\n",
       "      <th>Image_240</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>241 rows × 240 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Image_1 Image_2 Image_3 Image_4 Image_5 Image_6 Image_7 Image_8 Image_9  \\\n",
       "0       NaN      -1     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "1       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "2       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "3       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "4       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "236     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "237     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "238     NaN     NaN       0     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "239     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "240     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "    Image_10  ... Image_231 Image_232 Image_233 Image_234 Image_235 Image_236  \\\n",
       "0        NaN  ...       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "1        NaN  ...       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2        NaN  ...       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "3        NaN  ...       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "4        NaN  ...       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "..       ...  ...       ...       ...       ...       ...       ...       ...   \n",
       "236      NaN  ...       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "237      NaN  ...       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "238      NaN  ...       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "239      NaN  ...       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "240      NaN  ...       NaN       NaN       NaN       NaN       NaN         1   \n",
       "\n",
       "    Image_237 Image_238 Image_239 Image_240  \n",
       "0         NaN       NaN       NaN       NaN  \n",
       "1         NaN       NaN       NaN       NaN  \n",
       "2         NaN       NaN       NaN       NaN  \n",
       "3         NaN       NaN       NaN       NaN  \n",
       "4         NaN       NaN       NaN         1  \n",
       "..        ...       ...       ...       ...  \n",
       "236       NaN       NaN       NaN       NaN  \n",
       "237       NaN       NaN       NaN       NaN  \n",
       "238       NaN       NaN       NaN       NaN  \n",
       "239       NaN       NaN       NaN       NaN  \n",
       "240       NaN       NaN       NaN       NaN  \n",
       "\n",
       "[241 rows x 240 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(r\"/Users/Desktop/datafile/6_expert.csv\",encoding=\"gbk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
